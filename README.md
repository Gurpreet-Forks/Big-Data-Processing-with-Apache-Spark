[![GitHub issues](https://img.shields.io/github/issues/TrainingByPackt/Big-Data-Processing-with-Apache-Spark.svg)](https://github.com/TrainingByPackt/Big-Data-Processing-with-Apache-Spark/issues)
[![GitHub forks](https://img.shields.io/github/forks/TrainingByPackt/Big-Data-Processing-with-Apache-Spark.svg)](https://github.com/TrainingByPackt/Big-Data-Processing-with-Apache-Spark/network)
[![GitHub stars](https://img.shields.io/github/stars/TrainingByPackt/Big-Data-Processing-with-Apache-Spark.svg)](https://github.com/TrainingByPackt/Big-Data-Processing-with-Apache-Spark/stargazers)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/TrainingByPackt/Big-Data-Processing-with-Apache-Spark/pulls)

# Big Data Processing with Apache Spark
Processing big data in real time is challenging due to scalability, information consistency, and fault-tolerance. Big Data Processing with Apache Spark teaches you how to use Spark to make your overall analytical workflow faster and more efficient. You’ll explore all core concepts and tools within the Spark ecosystem, such as Spark Streaming, the Spark Streaming API, machine learning extension, and structured streaming.
You’ll begin by learning data processing fundamentals using Resilient Distributed Datasets (RDDs), SQL, Datasets, and Dataframes APIs. After grasping these fundamentals, you’ll move on to using Spark Streaming APIs to consume data in real time from TCP sockets, and integrate Amazon Web Services (AWS) for stream consumption.
By the end of this course, you’ll not only have understood how to use machine learning extensions and structured streams but you’ll also be able to apply Spark in your own upcoming big data projects.


## What you will learn
* Write  your  own  Python  programs  that  can  interact  with  Spark
* Implement  data  stream  consumption  using  Apache  Spark
* Recognize  common  operations  in  Spark  to  process  known  data  streams
* Integrate  Spark  streaming  with  Amazon  Web  Services 
* Create  a  collaborative  filtering  model  with  Python  and  the  movielens  dataset
* Apply  processed  data  streams  to  Spark  machine  learning  APIs

### Hardware requirements
For an optimal student experience, we recommend the following hardware configuration:
* **Processor**: 2.6 GHz or higher, preferably multi-core
* **Memory**: 4GB RAM
* **Hard disk**: 35 GB or more
* An Internet connection



### Software requirements
You’ll also need the following software installed in advance:
* Operating System: Windows (8 or higher)
*	PostgreSQL 9.0 or above
*	Python 3.0 or above
*	Spark 2.3.0
*	Apache Kafka 2.11.1 or above
*	Amazon Web Services (AWS) account 
